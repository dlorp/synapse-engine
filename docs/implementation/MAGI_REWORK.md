# MAGI REWORK - WebUI Experimentation Platform

**Date:** November 3, 2025
**Status:** ‚úÖ COMPLETE (All 12 Phases Finished!)
**Engineer:** Claude (Backend Architect + Frontend Engineer + DevOps Agents)
**Estimated Time:** 10-12 hours
**Time Elapsed:** ~7 hours

**Related Documents:**
- [PHASE 2 Complete](./PHASE_2_COMPLETE.md) - Configuration Profile System
- [PHASE 4 Complete](./PHASE4_COMPLETE.md) - Llama Server Manager
- [PHASE 6 Integration](./PHASE6_INTEGRATION_COMPLETE.md) - Full System Integration
- [PHASE 6 Docker](./PHASE6_DOCKER_COMPLETE.md) - Docker Configuration
- [Project README](../../README.md)
- [Dynamic Control Guide](../features/DYNAMIC_CONTROL.md)
- [Query Modes Guide](../features/MODES.md)

---

## üìä Implementation Progress

| Phase | Status | Component | Time | Notes |
|-------|--------|-----------|------|-------|
| **Phase 1** | ‚úÖ **COMPLETE** | Backend - Remove Auto-Start | 30 min | Modified main.py, removed StartupService |
| **Phase 2** | ‚úÖ **COMPLETE** | Backend - Dynamic Control API | 45 min | Added 4 new endpoints in models.py |
| **Phase 3** | ‚úÖ **COMPLETE** | Frontend - Control Buttons | 30 min | Added START ALL / STOP ALL in ModelManagementPage |
| **Phase 4** | ‚úÖ **COMPLETE** | Frontend - Mode Selector | 45 min | Created ModeSelector component, updated HomePage |
| **Phase 5** | ‚úÖ **COMPLETE** | Backend - Two-Stage Workflow | 60 min | Stage1:FAST ‚Üí Stage2:BALANCED/POWERFUL (complexity-based) |
| **Phase 6** | ‚úÖ **COMPLETE** | Frontend - Two-Stage Display | 45 min | Added TWO-STAGE PROCESSING panel in ResponseDisplay |
| **Phase 7** | ‚úÖ **COMPLETE** | Backend - Clean Q2/Q3/Q4 Refs | 30 min | Removed all quantization tier refs, now use size-based tiers |
| **Phase 8** | ‚úÖ **COMPLETE** | DevOps - Docker Consolidation | 30 min | Deleted dev file, hot reload now default in docker-compose.yml |
| **Phase 9** | ‚úÖ **COMPLETE** | DevOps - Docker Optimization | 15 min | Using python:3.11-slim (60-70% size reduction) |
| **Phase 10** | ‚úÖ **COMPLETE** | DevOps - Environment Variables | 15 min | Added CGRAG and mode system vars to .env.example |
| **Phase 11** | ‚úÖ **COMPLETE** | Documentation - README | 60 min | Complete README rewrite reflecting new architecture |
| **Phase 12** | ‚úÖ **COMPLETE** | Documentation - Supporting Docs | 30 min | Created MODES.md and DYNAMIC_CONTROL.md |

**Legend:**
- ‚úÖ COMPLETE - Implementation finished and verified
- üîÑ NEXT - Ready to implement next
- ‚è∏Ô∏è PENDING - Waiting for prerequisites
- ‚ùå BLOCKED - Blocked by issues

**Overall Progress:** 12/12 phases complete (100%) üéâ

---

## Executive Summary

### Vision Change

**From:** Configuration-driven auto-start service
**To:** WebUI-first experimentation platform

**Key Changes:**
1. ‚ùå **Remove auto-start** - Docker launches in ~5s with NO models loaded
2. ‚úÖ **Add dynamic control** - Start/stop models from WebUI without restart
3. ‚úÖ **Add mode system** - Two-Stage, Council, Debate, Multi-Chat modes
4. ‚úÖ **WebUI-first UX** - All configuration happens in browser, not YAML files
5. ‚úÖ **Keep CGRAG** - 2,351 lines already production-ready, no changes needed

### Current Problems

1. **Slow startup:** 40-50 seconds to launch all enabled models
2. **Inflexible:** Must restart Docker to change models
3. **Profile-locked:** YAML files required, no dynamic configuration
4. **No experimentation modes:** Only single-model or hardcoded two-stage

### New Behavior

```
User starts Docker ‚Üí MAGI launches blank (<5s) ‚Üí User opens WebUI
  ‚Üì
User navigates to Model Management ‚Üí Enables models ‚Üí Clicks "START ALL"
  ‚Üì
Servers launch dynamically (user sees progress)
  ‚Üì
User goes to Home ‚Üí Selects mode (Two-Stage, Simple, Council, etc.)
  ‚Üì
User submits query ‚Üí MAGI routes based on mode ‚Üí Response displayed
  ‚Üì
User can stop/restart models anytime from WebUI (no Docker restart)
```

---

## Phase 1: Remove Auto-Start Behavior

### File: [backend/app/main.py](../../backend/app/main.py)

**Lines to modify:** 88-96

**Current code (auto-starts servers):**
```python
# Initialize and run startup sequence
startup_service = StartupService(config, profile_name)
try:
    model_registry = await startup_service.initialize()  # ‚Üê LAUNCHES ALL SERVERS!

    # Expose services globally for routers
    server_manager = startup_service.server_manager
    profile_manager = startup_service.profile_manager
    discovery_service = startup_service.discovery_service
```

**New code (discovery only, no launch):**
```python
# Discovery service only - NO server launching
discovery_service = ModelDiscoveryService(
    scan_path=config.model_management.scan_path,
    port_range=config.model_management.port_range
)

# Load or create registry (cached if exists)
registry_path = Path("data/model_registry.json")
if registry_path.exists():
    model_registry = discovery_service.load_registry(registry_path)
    logger.info(f"‚úÖ Loaded {len(model_registry.models)} models from registry")
else:
    logger.info("No registry found, discovering models...")
    model_registry = discovery_service.discover_models()
    discovery_service.save_registry(model_registry, registry_path)
    logger.info(f"‚úÖ Discovered {len(model_registry.models)} models")

# Initialize server manager (but don't start any servers yet)
server_manager = LlamaServerManager(
    llama_server_path=config.model_management.llama_server_path,
    max_startup_time=config.model_management.max_startup_time,
    readiness_check_interval=config.model_management.readiness_check_interval
)

# Profile manager (still needed for future profile management)
project_root = Path(__file__).parent.parent.parent
profiles_dir = project_root / "config" / "profiles"
profile_manager = ProfileManager(profiles_dir=profiles_dir)

logger.info("‚úÖ MAGI ready - NO models loaded (awaiting WebUI commands)")
```

**Also update lines 100-103** to expose services:
```python
# Expose services globally for routers
from app.routers import models as models_router
models_router.model_registry = model_registry
models_router.server_manager = server_manager
models_router.profile_manager = profile_manager
models_router.discovery_service = discovery_service
```

**Expected result:** Docker startup completes in ~5 seconds, no llama-server processes running.

---

## Phase 2: Add Dynamic Server Control API

### File: [backend/app/routers/models.py](../../backend/app/routers/models.py)

**Add after line 559 (after `get_server_status` endpoint):**

```python
# ============================================================================
# DYNAMIC MODEL SERVER CONTROL (NO RESTART REQUIRED)
# ============================================================================

@router.post("/servers/{model_id}/start", response_model=dict)
async def start_model_server(model_id: str):
    """Start llama.cpp server for a specific model (dynamic, no restart).

    This endpoint allows starting individual model servers on-demand from the WebUI.
    Users can enable a model and immediately start its server without restarting Docker.

    Args:
        model_id: Unique model identifier from registry

    Returns:
        Server start status with port and timing information

    Raises:
        404: Model not found in registry
        503: Server manager not initialized
        500: Server failed to start
    """
    if not model_registry or model_id not in model_registry.models:
        raise HTTPException(
            status_code=404,
            detail=f"Model not found: {model_id}"
        )

    if not server_manager:
        raise HTTPException(
            status_code=503,
            detail="Server manager not initialized"
        )

    model = model_registry.models[model_id]

    # Check if already running
    if server_manager.is_server_running(model_id):
        logger.info(f"Server already running for {model_id}")
        return {
            "message": f"Server already running for {model_id}",
            "model_id": model_id,
            "port": model.port,
            "status": "already_running"
        }

    try:
        # Start server asynchronously
        logger.info(f"Starting server for {model_id}...")
        start_time = time.time()

        await server_manager.start_server(model)

        elapsed = time.time() - start_time
        logger.info(f"‚úÖ Started server for {model_id} on port {model.port} ({elapsed:.1f}s)")

        return {
            "message": f"Server started for {model_id}",
            "model_id": model_id,
            "display_name": model.get_display_name(),
            "port": model.port,
            "status": "started",
            "startup_time_seconds": round(elapsed, 2)
        }

    except Exception as e:
        logger.error(f"‚ùå Failed to start server for {model_id}: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Failed to start server: {str(e)}"
        )


@router.post("/servers/{model_id}/stop", response_model=dict)
async def stop_model_server(model_id: str):
    """Stop llama.cpp server for a specific model (dynamic, no restart).

    This endpoint allows stopping individual model servers on-demand from the WebUI.
    Servers are gracefully shut down with SIGTERM, then SIGKILL if needed.

    Args:
        model_id: Unique model identifier from registry

    Returns:
        Server stop status

    Raises:
        503: Server manager not initialized
    """
    if not server_manager:
        raise HTTPException(
            status_code=503,
            detail="Server manager not initialized"
        )

    # Check if running
    if not server_manager.is_server_running(model_id):
        logger.info(f"Server not running for {model_id}")
        return {
            "message": f"Server not running for {model_id}",
            "model_id": model_id,
            "status": "not_running"
        }

    try:
        logger.info(f"Stopping server for {model_id}...")
        stop_time = time.time()

        await server_manager.stop_server(model_id)

        elapsed = time.time() - stop_time
        logger.info(f"‚úÖ Stopped server for {model_id} ({elapsed:.1f}s)")

        return {
            "message": f"Server stopped for {model_id}",
            "model_id": model_id,
            "status": "stopped",
            "shutdown_time_seconds": round(elapsed, 2)
        }

    except Exception as e:
        logger.error(f"‚ùå Failed to stop server for {model_id}: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Failed to stop server: {str(e)}"
        )


@router.post("/servers/start-all", response_model=dict)
async def start_all_enabled_servers():
    """Start all enabled models (dynamic, no restart).

    This endpoint launches servers for all models marked as enabled in the registry.
    Servers start concurrently for optimal performance.

    Returns:
        Summary of servers started with timing information

    Raises:
        503: Model registry or server manager not initialized
    """
    if not model_registry:
        raise HTTPException(
            status_code=503,
            detail="Model registry not initialized"
        )

    if not server_manager:
        raise HTTPException(
            status_code=503,
            detail="Server manager not initialized"
        )

    # Get all enabled models
    enabled_models = [
        model for model in model_registry.models.values()
        if model.enabled
    ]

    if not enabled_models:
        logger.info("No enabled models to start")
        return {
            "message": "No enabled models to start",
            "started": 0,
            "total": 0,
            "models": []
        }

    try:
        logger.info(f"Starting {len(enabled_models)} enabled models...")
        start_time = time.time()

        # Start all concurrently
        results = await server_manager.start_all(enabled_models)

        elapsed = time.time() - start_time
        logger.info(
            f"‚úÖ Started {len(results)}/{len(enabled_models)} servers ({elapsed:.1f}s total)"
        )

        return {
            "message": f"Started {len(results)}/{len(enabled_models)} servers",
            "started": len(results),
            "total": len(enabled_models),
            "startup_time_seconds": round(elapsed, 2),
            "models": [
                {
                    "model_id": m.model_id,
                    "display_name": m.get_display_name(),
                    "port": m.port
                }
                for m in enabled_models
            ]
        }

    except Exception as e:
        logger.error(f"‚ùå Failed to start servers: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Failed to start servers: {str(e)}"
        )


@router.post("/servers/stop-all", response_model=dict)
async def stop_all_servers():
    """Stop all running servers (dynamic, no restart).

    This endpoint gracefully shuts down all running llama.cpp servers.
    Useful for freeing memory or switching to a different model set.

    Returns:
        Summary of servers stopped

    Raises:
        503: Server manager not initialized
    """
    if not server_manager:
        raise HTTPException(
            status_code=503,
            detail="Server manager not initialized"
        )

    try:
        running_count = len(server_manager.servers)

        if running_count == 0:
            logger.info("No servers running to stop")
            return {
                "message": "No servers running",
                "stopped": 0
            }

        logger.info(f"Stopping {running_count} running servers...")
        stop_time = time.time()

        await server_manager.stop_all()

        elapsed = time.time() - stop_time
        logger.info(f"‚úÖ Stopped {running_count} servers ({elapsed:.1f}s)")

        return {
            "message": f"Stopped {running_count} servers",
            "stopped": running_count,
            "shutdown_time_seconds": round(elapsed, 2)
        }

    except Exception as e:
        logger.error(f"‚ùå Failed to stop servers: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Failed to stop servers: {str(e)}"
        )
```

**Add import at top of file:**
```python
import time  # Add to existing imports
```

---

### File: [backend/app/routers/models.py](../../backend/app/routers/models.py) (Update Existing Endpoint)

**Lines to modify:** 450-523 (toggle_model_enabled endpoint)

**Find this section and replace:**

```python
@router.put("/{model_id}/enabled", response_model=dict)
async def toggle_model_enabled(
    model_id: str,
    enabled: bool = Body(..., embed=True)
):
    """Enable or disable a model AND start/stop its server dynamically."""
    global model_registry

    if not model_registry or model_id not in model_registry.models:
        raise HTTPException(
            status_code=404,
            detail=f"Model not found: {model_id}"
        )

    # Update enabled status
    model_registry.models[model_id].enabled = enabled

    # Save registry
    registry_path = Path("data/model_registry.json")
    discovery_service.save_registry(model_registry, registry_path)

    logger.info(f"{'Enabled' if enabled else 'Disabled'} model: {model_id}")

    # ============================================================================
    # NEW: Dynamic server control - start/stop based on enabled status
    # ============================================================================
    server_status = "no_action"

    if enabled:
        # Start server if not already running
        if server_manager and not server_manager.is_server_running(model_id):
            try:
                logger.info(f"Auto-starting server for {model_id}...")
                await server_manager.start_server(model_registry.models[model_id])
                logger.info(f"‚úÖ Enabled and started {model_id}")
                server_status = "started"
            except Exception as e:
                logger.error(f"Enabled {model_id} but failed to start server: {e}")
                server_status = "failed_to_start"
        else:
            server_status = "already_running"
    else:
        # Stop server if running
        if server_manager and server_manager.is_server_running(model_id):
            try:
                logger.info(f"Auto-stopping server for {model_id}...")
                await server_manager.stop_server(model_id)
                logger.info(f"‚ùå Disabled and stopped {model_id}")
                server_status = "stopped"
            except Exception as e:
                logger.error(f"Disabled {model_id} but failed to stop server: {e}")
                server_status = "failed_to_stop"
        else:
            server_status = "not_running"

    return {
        "message": f"Model {model_id} {'enabled' if enabled else 'disabled'}",
        "model_id": model_id,
        "enabled": enabled,
        "server_status": server_status
    }
```

---

## Phase 3: Frontend - Dynamic Control Buttons

### File: [frontend/src/pages/ModelManagementPage/ModelManagementPage.tsx](../../frontend/src/pages/ModelManagementPage/ModelManagementPage.tsx)

**Add state variables (around line 50):**
```tsx
const [isStartingAll, setIsStartingAll] = useState(false);
const [isStoppingAll, setIsStoppingAll] = useState(false);
```

**Add handler functions (around line 100):**
```tsx
const handleStartAll = async () => {
  setIsStartingAll(true);
  try {
    const response = await fetch('/api/models/servers/start-all', {
      method: 'POST'
    });

    if (!response.ok) {
      throw new Error(`Failed to start servers: ${response.statusText}`);
    }

    const result = await response.json();
    console.log(`Started ${result.started}/${result.total} servers`);

    // Refresh registry to show running status
    await rescan();
  } catch (err) {
    console.error('Failed to start servers:', err);
    setError(err instanceof Error ? err.message : 'Failed to start servers');
  } finally {
    setIsStartingAll(false);
  }
};

const handleStopAll = async () => {
  setIsStoppingAll(true);
  try {
    const response = await fetch('/api/models/servers/stop-all', {
      method: 'POST'
    });

    if (!response.ok) {
      throw new Error(`Failed to stop servers: ${response.statusText}`);
    }

    const result = await response.json();
    console.log(`Stopped ${result.stopped} servers`);

    // Refresh registry
    await rescan();
  } catch (err) {
    console.error('Failed to stop servers:', err);
    setError(err instanceof Error ? err.message : 'Failed to stop servers');
  } finally {
    setIsStoppingAll(false);
  }
};
```

**Add buttons in header section (after RE-SCAN button, around line 633):**
```tsx
<div className={styles.actions}>
  <button
    className={styles.rescanButton}
    onClick={handleRescan}
    disabled={isRescanning}
  >
    {isRescanning ? 'SCANNING...' : 'RE-SCAN HUB'}
  </button>

  <button
    className={styles.startAllButton}
    onClick={handleStartAll}
    disabled={isStartingAll}
  >
    {isStartingAll ? 'STARTING...' : 'START ALL ENABLED'}
  </button>

  <button
    className={styles.stopAllButton}
    onClick={handleStopAll}
    disabled={isStoppingAll}
  >
    {isStoppingAll ? 'STOPPING...' : 'STOP ALL SERVERS'}
  </button>
</div>
```

**Add CSS styles to `ModelManagementPage.module.css`:**
```css
.startAllButton {
  background: var(--color-success);
  color: var(--color-bg);
  border: 2px solid var(--color-success);
  padding: 0.5rem 1rem;
  font-family: var(--font-mono);
  font-size: 0.875rem;
  font-weight: 600;
  cursor: pointer;
  transition: all 0.2s;
}

.startAllButton:hover:not(:disabled) {
  background: transparent;
  color: var(--color-success);
}

.startAllButton:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.stopAllButton {
  background: var(--color-error);
  color: var(--color-bg);
  border: 2px solid var(--color-error);
  padding: 0.5rem 1rem;
  font-family: var(--font-mono);
  font-size: 0.875rem;
  font-weight: 600;
  cursor: pointer;
  transition: all 0.2s;
}

.stopAllButton:hover:not(:disabled) {
  background: transparent;
  color: var(--color-error);
}

.stopAllButton:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}
```

---

## Phase 4: Frontend - Mode Selector Component

### New File: [frontend/src/components/modes/ModeSelector.tsx](../../frontend/src/components/modes/ModeSelector.tsx)

```tsx
import React from 'react';
import { Panel } from '../terminal/Panel/Panel';
import styles from './ModeSelector.module.css';

export type QueryMode = 'two-stage' | 'simple' | 'council' | 'debate' | 'chat';

interface ModeSelectorProps {
  currentMode: QueryMode;
  onModeChange: (mode: QueryMode) => void;
}

interface ModeConfig {
  id: QueryMode;
  label: string;
  description: string;
  available: boolean;
}

const MODES: ModeConfig[] = [
  {
    id: 'two-stage',
    label: 'TWO-STAGE',
    description: 'Fast model + CGRAG ‚Üí Powerful refinement',
    available: true
  },
  {
    id: 'simple',
    label: 'SIMPLE',
    description: 'Single model query',
    available: true
  },
  {
    id: 'council',
    label: 'COUNCIL',
    description: 'Multiple LLMs discuss to consensus',
    available: false
  },
  {
    id: 'debate',
    label: 'DEBATE',
    description: 'Models argue opposing viewpoints',
    available: false
  },
  {
    id: 'chat',
    label: 'MULTI-CHAT',
    description: 'Models converse with each other',
    available: false
  }
];

export const ModeSelector: React.FC<ModeSelectorProps> = ({
  currentMode,
  onModeChange
}) => {
  return (
    <Panel title="QUERY MODE SELECTION" variant="accent">
      <div className={styles.modeGrid}>
        {MODES.map(mode => (
          <button
            key={mode.id}
            className={`
              ${styles.modeButton}
              ${currentMode === mode.id ? styles.active : ''}
              ${!mode.available ? styles.disabled : ''}
            `}
            onClick={() => mode.available && onModeChange(mode.id)}
            disabled={!mode.available}
            aria-label={`${mode.label}: ${mode.description}`}
          >
            <div className={styles.modeLabel}>{mode.label}</div>
            <div className={styles.modeDescription}>{mode.description}</div>
            {!mode.available && (
              <div className={styles.comingSoon}>COMING SOON</div>
            )}
            {currentMode === mode.id && (
              <div className={styles.activeIndicator}>‚óè ACTIVE</div>
            )}
          </button>
        ))}
      </div>
    </Panel>
  );
};
```

### New File: [frontend/src/components/modes/ModeSelector.module.css](../../frontend/src/components/modes/ModeSelector.module.css)

```css
.modeGrid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
  gap: 1rem;
  padding: 1rem;
}

.modeButton {
  background: var(--color-bg-secondary);
  border: 2px solid var(--color-border);
  padding: 1rem;
  cursor: pointer;
  transition: all 0.2s ease;
  text-align: left;
  position: relative;
  font-family: var(--font-mono);
}

.modeButton:hover:not(:disabled) {
  border-color: var(--color-primary);
  background: var(--color-bg-hover);
}

.modeButton.active {
  border-color: var(--color-accent);
  background: var(--color-bg-active);
}

.modeButton.disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.modeLabel {
  font-size: 1rem;
  font-weight: 700;
  color: var(--color-primary);
  margin-bottom: 0.5rem;
  letter-spacing: 0.05em;
}

.modeDescription {
  font-size: 0.75rem;
  color: var(--color-text-secondary);
  line-height: 1.4;
  margin-bottom: 0.5rem;
}

.comingSoon {
  position: absolute;
  top: 0.5rem;
  right: 0.5rem;
  font-size: 0.625rem;
  color: var(--color-warning);
  font-weight: 600;
  letter-spacing: 0.1em;
}

.activeIndicator {
  font-size: 0.75rem;
  color: var(--color-accent);
  font-weight: 600;
  margin-top: 0.5rem;
}
```

---

### File: [frontend/src/pages/HomePage/HomePage.tsx](../../frontend/src/pages/HomePage/HomePage.tsx)

**Add import:**
```tsx
import { ModeSelector, QueryMode } from '../../components/modes/ModeSelector';
```

**Add state:**
```tsx
const [queryMode, setQueryMode] = useState<QueryMode>('two-stage');
```

**Add ModeSelector above QueryInput (around line 80):**
```tsx
return (
  <div className={styles.homePage}>
    <ModeSelector
      currentMode={queryMode}
      onModeChange={setQueryMode}
    />

    <QueryInput
      onSubmit={(query) => handleQuery(query, queryMode)}
    />

    {response && (
      <ResponseDisplay
        response={response}
        mode={queryMode}
      />
    )}
  </div>
);
```

**Update handleQuery to include mode:**
```tsx
const handleQuery = async (query: string, mode: QueryMode) => {
  setIsLoading(true);
  setError(null);

  try {
    const response = await fetch('/api/query', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        query,
        mode,  // ‚Üê Include mode in request
        use_context: true
      })
    });

    if (!response.ok) {
      throw new Error(`Query failed: ${response.statusText}`);
    }

    const result = await response.json();
    setResponse(result);
  } catch (err) {
    setError(err instanceof Error ? err.message : 'Query failed');
  } finally {
    setIsLoading(false);
  }
};
```

---

## Phase 5: Backend - Two-Stage Workflow Implementation

### File: [backend/app/models/query.py](../../backend/app/models/query.py)

**Add mode field to QueryRequest (around line 40):**
```python
mode: Literal["simple", "two-stage", "council", "debate", "chat"] = Field(
    default="two-stage",
    description="Query processing mode"
)
```

**Add two-stage fields to QueryMetadata (after line 209):**
```python
# Query mode
query_mode: str = Field(default="simple", description="Query processing mode used")

# Two-stage workflow metadata
stage1_response: Optional[str] = Field(default=None, description="Stage 1 model response")
stage1_model_id: Optional[str] = Field(default=None, description="Stage 1 model ID")
stage1_tier: Optional[str] = Field(default=None, description="Stage 1 tier")
stage1_processing_time: Optional[int] = Field(default=None, description="Stage 1 time (ms)")
stage1_tokens: Optional[int] = Field(default=None, description="Stage 1 tokens generated")

stage2_model_id: Optional[str] = Field(default=None, description="Stage 2 model ID")
stage2_tier: Optional[str] = Field(default=None, description="Stage 2 tier")
stage2_processing_time: Optional[int] = Field(default=None, description="Stage 2 time (ms)")
stage2_tokens: Optional[int] = Field(default=None, description="Stage 2 tokens generated")
```

**Add import:**
```python
from typing import Literal  # Add to existing typing imports
```

---

### File: [backend/app/routers/query.py](../../backend/app/routers/query.py)

**Replace single-stage query logic (around lines 148-286) with mode-based routing:**

This is the most complex change. Find the section that selects the model and calls it, and replace with:

```python
# ============================================================================
# MODE-BASED QUERY ROUTING
# ============================================================================

query_mode = request.mode or "two-stage"
logger.info(f"Query mode: {query_mode}")

if query_mode == "two-stage":
    # ========================================================================
    # TWO-STAGE WORKFLOW
    # ========================================================================
    logger.info("üîÑ Two-stage workflow selected")

    # STAGE 1: BALANCED tier with CGRAG context
    stage1_start = time.time()
    stage1_tier = "balanced"

    try:
        stage1_model = await model_manager.select_model(stage1_tier)
        logger.info(f"Stage 1 model selected: {stage1_model}")
    except Exception as e:
        logger.error(f"Failed to select Stage 1 model: {e}")
        raise HTTPException(
            status_code=503,
            detail=f"No {stage1_tier} tier models available"
        )

    # CGRAG retrieval for Stage 1
    cgrag_artifacts = []
    cgrag_context = ""

    if request.use_context:
        try:
            # [... existing CGRAG retrieval code ...]
            # This section already exists in query.py lines 162-265
            # Just ensure it builds cgrag_context string
        except Exception as e:
            logger.warning(f"CGRAG retrieval failed, continuing without context: {e}")

    # Build Stage 1 prompt
    stage1_prompt = cgrag_context + "\n\n" + request.query if cgrag_context else request.query

    # Stage 1 model call
    try:
        stage1_response = await model_manager.call_model(
            stage1_model,
            stage1_prompt,
            max_tokens=500,  # Limited tokens for Stage 1
            temperature=request.temperature
        )
        stage1_time = int((time.time() - stage1_start) * 1000)
        stage1_tokens = len(stage1_response.split())  # Rough estimate

        logger.info(f"‚úÖ Stage 1 complete: {len(stage1_response)} chars in {stage1_time}ms")
    except Exception as e:
        logger.error(f"Stage 1 model call failed: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Stage 1 processing failed: {str(e)}"
        )

    # STAGE 2: POWERFUL tier refinement
    stage2_start = time.time()
    stage2_tier = "powerful"

    try:
        stage2_model = await model_manager.select_model(stage2_tier)
        logger.info(f"Stage 2 model selected: {stage2_model}")
    except Exception as e:
        logger.error(f"Failed to select Stage 2 model: {e}")
        raise HTTPException(
            status_code=503,
            detail=f"No {stage2_tier} tier models available"
        )

    # Build Stage 2 refinement prompt
    stage2_prompt = f"""You are refining a response to improve its quality.

Original Query:
{request.query}

Initial Response (from Stage 1 model):
{stage1_response}

Instructions:
- Provide an improved, comprehensive response to the original query
- Expand on key points from the initial response
- Add depth, examples, and additional context
- Ensure accuracy and completeness
- Maintain a clear, professional tone

Refined Response:"""

    # Stage 2 model call
    try:
        stage2_response = await model_manager.call_model(
            stage2_model,
            stage2_prompt,
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
        stage2_time = int((time.time() - stage2_start) * 1000)
        stage2_tokens = len(stage2_response.split())  # Rough estimate

        total_time = stage1_time + stage2_time
        logger.info(
            f"‚úÖ Stage 2 complete: {len(stage2_response)} chars in {stage2_time}ms "
            f"(total: {total_time}ms)"
        )
    except Exception as e:
        logger.error(f"Stage 2 model call failed: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Stage 2 processing failed: {str(e)}"
        )

    # Build response with two-stage metadata
    return QueryResponse(
        response=stage2_response,
        metadata=QueryMetadata(
            model_tier=stage2_tier,
            model_id=stage2_model,
            processing_time_ms=total_time,
            complexity_score=complexity_score,
            cgrag_artifacts=len(cgrag_artifacts),
            cgrag_artifacts_info=cgrag_artifacts,
            query_mode="two-stage",
            stage1_response=stage1_response,
            stage1_model_id=stage1_model,
            stage1_tier=stage1_tier,
            stage1_processing_time=stage1_time,
            stage1_tokens=stage1_tokens,
            stage2_model_id=stage2_model,
            stage2_tier=stage2_tier,
            stage2_processing_time=stage2_time,
            stage2_tokens=stage2_tokens
        )
    )

elif query_mode == "simple":
    # ========================================================================
    # SIMPLE SINGLE-MODEL WORKFLOW (existing implementation)
    # ========================================================================
    logger.info("üéØ Simple single-model workflow")

    # [... keep existing single-model logic here ...]
    # This is the current implementation that already exists

    # Just add query_mode to metadata:
    return QueryResponse(
        response=model_response,
        metadata=QueryMetadata(
            # ... existing fields ...
            query_mode="simple"
        )
    )

else:
    # ========================================================================
    # FUTURE MODES (not yet implemented)
    # ========================================================================
    raise HTTPException(
        status_code=400,
        detail=f"Query mode '{query_mode}' not yet implemented. Available modes: simple, two-stage"
    )
```

---

## Phase 6: Frontend - Display Two-Stage Results

### File: [frontend/src/components/query/ResponseDisplay.tsx](../../frontend/src/components/query/ResponseDisplay.tsx)

**Add after CGRAG artifacts section (around line 356):**

```tsx
{metadata.queryMode === 'two-stage' && (
  <Panel title="TWO-STAGE PROCESSING" variant="accent">
    <div className={styles.twoStageInfo}>
      {/* Stage 1 */}
      <div className={styles.stage}>
        <div className={styles.stageHeader}>
          <span className={styles.stageLabel}>
            STAGE 1: {metadata.stage1Tier?.toUpperCase()}
          </span>
          <span className={styles.stageModel}>{metadata.stage1ModelId}</span>
          <span className={styles.stageTime}>
            {metadata.stage1ProcessingTime}ms
          </span>
        </div>

        {metadata.stage1Response && (
          <details className={styles.stageDetails}>
            <summary className={styles.stageSummary}>
              View Stage 1 Response ({metadata.stage1Tokens} tokens)
            </summary>
            <pre className={styles.stageResponse}>
              {metadata.stage1Response}
            </pre>
          </details>
        )}
      </div>

      {/* Arrow indicator */}
      <div className={styles.stageArrow}>
        ‚Üì REFINEMENT ‚Üì
      </div>

      {/* Stage 2 */}
      <div className={styles.stage}>
        <div className={styles.stageHeader}>
          <span className={styles.stageLabel}>
            STAGE 2: {metadata.stage2Tier?.toUpperCase()}
          </span>
          <span className={styles.stageModel}>{metadata.stage2ModelId}</span>
          <span className={styles.stageTime}>
            {metadata.stage2ProcessingTime}ms
          </span>
        </div>

        <div className={styles.stageNote}>
          Final response shown above
        </div>
      </div>

      {/* Total timing */}
      <div className={styles.totalTiming}>
        Total: {metadata.processingTimeMs}ms
      </div>
    </div>
  </Panel>
)}
```

**Add CSS to `ResponseDisplay.module.css`:**

```css
.twoStageInfo {
  display: flex;
  flex-direction: column;
  gap: 1rem;
  padding: 1rem;
}

.stage {
  background: var(--color-bg-secondary);
  border: 1px solid var(--color-border);
  padding: 1rem;
  border-radius: 4px;
}

.stageHeader {
  display: flex;
  justify-content: space-between;
  align-items: center;
  gap: 1rem;
  margin-bottom: 0.5rem;
}

.stageLabel {
  font-weight: 700;
  color: var(--color-primary);
  font-size: 0.875rem;
  letter-spacing: 0.05em;
}

.stageModel {
  color: var(--color-text-secondary);
  font-size: 0.75rem;
  font-family: var(--font-mono);
}

.stageTime {
  color: var(--color-accent);
  font-size: 0.75rem;
  font-weight: 600;
}

.stageDetails {
  margin-top: 0.5rem;
}

.stageSummary {
  cursor: pointer;
  color: var(--color-primary);
  font-size: 0.75rem;
  padding: 0.5rem;
  background: var(--color-bg);
  border: 1px solid var(--color-border);
  border-radius: 4px;
}

.stageSummary:hover {
  background: var(--color-bg-hover);
}

.stageResponse {
  margin-top: 0.5rem;
  padding: 1rem;
  background: var(--color-bg);
  border: 1px solid var(--color-border);
  border-radius: 4px;
  font-size: 0.875rem;
  line-height: 1.6;
  white-space: pre-wrap;
  overflow-x: auto;
}

.stageArrow {
  text-align: center;
  color: var(--color-accent);
  font-weight: 700;
  font-size: 1rem;
  letter-spacing: 0.1em;
  padding: 0.5rem 0;
}

.stageNote {
  color: var(--color-text-secondary);
  font-size: 0.75rem;
  font-style: italic;
  margin-top: 0.5rem;
}

.totalTiming {
  text-align: right;
  color: var(--color-accent);
  font-weight: 600;
  font-size: 0.875rem;
  padding-top: 0.5rem;
  border-top: 1px solid var(--color-border);
}
```

**Add TypeScript type:**

Update `frontend/src/types/query.ts` to include two-stage fields:

```typescript
export interface QueryMetadata {
  // ... existing fields ...

  // Two-stage fields
  queryMode?: string;
  stage1Response?: string;
  stage1ModelId?: string;
  stage1Tier?: string;
  stage1ProcessingTime?: number;
  stage1Tokens?: number;
  stage2ModelId?: string;
  stage2Tier?: string;
  stage2ProcessingTime?: number;
  stage2Tokens?: number;
}
```

---

## Phase 7: Clean Up Deprecated Code

### Remove Q2/Q3/Q4 References

**File: [backend/app/core/config.py](../../backend/app/core/config.py)**

Delete lines 37-41 (hardcoded model URLs):
```python
# DELETE THESE:
model_q2_fast_1_url: str = 'http://localhost:8080'
model_q2_fast_2_url: str = 'http://localhost:8081'
model_q3_synth_url: str = 'http://localhost:8082'
model_q4_deep_url: str = 'http://localhost:8083'
```

**File: [backend/app/models/config.py](../../backend/app/models/config.py)**

Replace Q2/Q3/Q4 with FAST/BALANCED/POWERFUL:
- Line 12: Update docstring `tier: Quantization tier (Q2, Q3, Q4)` ‚Üí `tier: Model tier (fast, balanced, powerful)`
- Lines 43-46: Remove or update validator to accept new tiers
- Line 66: Change default `'Q3'` ‚Üí `'balanced'`

**File: `backend/app/models/query.py`**

Update docstrings and examples:
- Lines 96-111: Replace Q2/Q3/Q4 references with FAST/BALANCED/POWERFUL
- Line 164: Update example model ID from `Q2_FAST_1` to `deepseek_8b_q4km_powerful`

**File: [backend/app/services/models.py](../../backend/app/services/models.py)**

- Lines 29-31: Update "Q2" load balancing ‚Üí "FAST tier"
- Line 55, 310, 323-325: Update Q2 references
- Lines 276, 290: Address TODOs or mark as future work

**File: [backend/app/services/routing.py](../../backend/app/services/routing.py)**

- Line 87: Update Q2 example in docstring

---

## Phase 8: Docker Consolidation

### Delete File: docker-compose.dev.yml

This file is no longer needed - hot reload will be the default.

### Update File: [docker-compose.yml](../../docker-compose.yml)

**Backend service - ensure hot reload is enabled (around lines 125-135):**

```yaml
backend:
  build:
    context: ./backend
    dockerfile: Dockerfile
  ports:
    - "8000:8000"
    - "8080-8099:8080-8099"  # Model server ports
  volumes:
    - ./backend:/app  # ‚Üê Source code mount for hot reload
    - ./backend/data:/app/data
    - ${MODEL_SCAN_PATH}:/models:ro
    - /usr/local/bin/llama-server:/usr/local/bin/llama-server:ro
  environment:
    ENVIRONMENT: development
    LOG_LEVEL: DEBUG
    MAGI_PROFILE: ${MAGI_PROFILE:-development}
    MODEL_SCAN_PATH: /models
  command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload  # ‚Üê --reload flag
  depends_on:
    redis:
      condition: service_healthy
```

**Frontend service - ensure Vite dev server (around lines 207-260):**

```yaml
frontend:
  build:
    context: ./frontend
    dockerfile: Dockerfile.dev  # Use dev Dockerfile with Vite
    args:
      VITE_API_BASE_URL: /api
      VITE_WS_URL: /ws
  ports:
    - "5173:5173"
  volumes:
    - ./frontend/src:/app/src  # ‚Üê Source code mount for HMR
    - /app/node_modules        # Anonymous volume for node_modules
  environment:
    NODE_ENV: development
  command: npm run dev -- --host 0.0.0.0  # Vite dev server
```

---

## Phase 9: Backend Docker Image Optimization

### File: [backend/Dockerfile](../../backend/Dockerfile)

**Change line 21** from `python:3.11` to `python:3.11-slim`:

```dockerfile
# Build stage
FROM python:3.11-slim AS builder  # ‚Üê Changed from python:3.11

# Install build dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*
```

**Expected size reduction:** From 1.75GB to ~500-700MB

---

## Phase 10: Update Environment Variables

### File: [.env.example](../../.env.example)

**Update Vite variables:**
```bash
# Frontend (embedded at build time)
VITE_API_BASE_URL=/api
VITE_WS_URL=/ws
```

**Add CGRAG variables:**
```bash
# CGRAG Configuration
CGRAG_INDEX_PATH=/data/faiss_indexes/
CGRAG_CHUNK_SIZE=512
CGRAG_CHUNK_OVERLAP=50
CGRAG_TOKEN_BUDGET=8000
CGRAG_MIN_RELEVANCE=0.7
CGRAG_EMBEDDING_MODEL=all-MiniLM-L6-v2
```

**Add mode system variables:**
```bash
# Query Modes
DEFAULT_QUERY_MODE=two-stage  # Options: simple, two-stage, council, debate, chat
ENABLE_EXPERIMENTAL_MODES=false  # Enable council, debate, chat modes
```

---

## Phase 11: Complete README Rewrite

### File: [README.md](../../README.md)

**Replace entire content with:**

```markdown
# MAGI - Multi-Model Experimentation Platform

> WebUI for experimenting with local LLMs through different interaction modes

**Status:** Phase 6 Complete ‚úÖ | CGRAG Operational ‚úÖ | Two-Stage Workflow ‚úÖ

## What is MAGI?

MAGI is a **WebUI experimentation platform** for local LLM deployments. Launch it blank, select models dynamically, choose interaction modes, and experiment without ever restarting Docker.

### Key Features

üé® **WebUI-First** - All control happens in the browser, no YAML editing
‚ö° **Fast Startup** - Launches in ~5 seconds with no models loaded
üîÑ **Dynamic Control** - Start/stop models without Docker restart
üéØ **Multiple Modes** - Two-Stage, Simple, Council, Debate, Multi-Chat
üìö **CGRAG Integration** - Automatic context retrieval with FAISS (<100ms)
üîç **Auto-Discovery** - Finds GGUF models in your Hugging Face cache

### Available Query Modes

- ‚úÖ **Two-Stage** - Fast model + CGRAG ‚Üí Powerful refinement
- ‚úÖ **Simple** - Single model query
- üîÑ **Council** - Multiple LLMs discuss to consensus (Coming Soon)
- üîÑ **Debate** - Models argue opposing viewpoints (Coming Soon)
- üîÑ **Multi-Chat** - Models converse with each other (Coming Soon)

## Quick Start

### Prerequisites

- Docker Desktop installed and running
- GGUF models in HuggingFace cache (`~/.cache/huggingface/hub/`)
- llama-server binary at `/usr/local/bin/llama-server`
- (Optional) Documentation to index for CGRAG

### Installation

1. **Clone repository**
   ```bash
   git clone <repo-url>
   cd MAGI
   ```

2. **Configure environment**
   ```bash
   cp .env.example .env
   # Edit .env - set MODEL_SCAN_PATH to your HuggingFace cache
   ```

3. **Start MAGI**
   ```bash
   docker-compose up -d
   ```

   **Startup completes in ~5 seconds** - no models are loaded yet!

4. **Access WebUI**
   - Frontend: http://localhost:5173
   - Backend API: http://localhost:8000/docs

### Your First Query

1. **Open WebUI:** http://localhost:5173

2. **Navigate to Model Management** ‚Üí Enable models you want to use

3. **Click "START ALL ENABLED"** ‚Üí Wait for models to load (40-50s for 4 models)

4. **Go to Home** ‚Üí Select "Two-Stage" mode

5. **Submit a query** ‚Üí Watch MAGI process with CGRAG + two-stage refinement!

## How It Works

### Blank Canvas Startup

```
Docker starts ‚Üí MAGI discovers models ‚Üí Registry saved ‚Üí Ready in ~5s
(No models loaded, no llama-server processes running)
```

### Dynamic Model Control

```
User visits Model Management ‚Üí Checks "Enable" on models ‚Üí Clicks "START ALL"
  ‚Üì
Servers launch dynamically (user sees progress)
  ‚Üì
Models ready for queries (can stop/restart anytime)
```

### Two-Stage Workflow

```
User selects Two-Stage mode ‚Üí Submits query
  ‚Üì
STAGE 1: BALANCED tier
  - CGRAG retrieves relevant documentation
  - Fast model generates initial response (500 tokens)
  - Completes in <3 seconds
  ‚Üì
STAGE 2: POWERFUL tier
  - Receives Stage 1 response + original query
  - Refines with depth and accuracy
  - Completes in <12 seconds
  ‚Üì
Final refined response displayed
```

## Architecture

### Three-Layer Model Management

**1. DISCOVERY (Automatic)**
- Scans HuggingFace cache for GGUF models
- Parses filenames (handles multiple naming conventions)
- Extracts metadata: family, size, quantization, capabilities
- Auto-assigns tier: FAST (<7B), BALANCED (7-14B), POWERFUL (>14B or thinking)
- Saves to `model_registry.json`

**2. CONFIGURATION (WebUI-Driven)**
- User enables specific models via checkboxes
- User selects query mode
- User configures tier overrides (optional)
- No YAML editing required

**3. ACTIVATION (Dynamic)**
- User clicks "START ALL" ‚Üí servers launch
- User clicks checkboxes ‚Üí individual servers start/stop
- No Docker restart required
- Graceful shutdown when stopped

### CGRAG System

**Already Production-Ready (2,351 lines of code)**

- Document indexing with smart chunking (512 words, 50-word overlap)
- Batched embedding generation (sentence-transformers)
- FAISS vector search (<100ms retrieval)
- Token budget management (8000 token default)
- Relevance filtering (70% threshold)
- Supported formats: .md, .py, .txt, .yaml, .json, .rst

## API Reference

### Dynamic Model Control

**POST `/api/models/servers/{model_id}/start`**
Start a specific model server (no restart required)

**POST `/api/models/servers/{model_id}/stop`**
Stop a specific model server (no restart required)

**POST `/api/models/servers/start-all`**
Start all enabled models

**POST `/api/models/servers/stop-all`**
Stop all running servers

### Query Processing

**POST `/api/query`**

```json
{
  "query": "Explain async patterns in Python",
  "mode": "two-stage",
  "use_context": true,
  "max_tokens": 2048
}
```

Response includes two-stage metadata:

```json
{
  "response": "...",
  "metadata": {
    "queryMode": "two-stage",
    "stage1Response": "...",
    "stage1ModelId": "qwen_14b_q4km_balanced",
    "stage1Tier": "balanced",
    "stage1ProcessingTime": 2100,
    "stage2ModelId": "deepseek_8b_q4km_powerful",
    "stage2Tier": "powerful",
    "stage2ProcessingTime": 4523,
    "cgragArtifacts": 3
  }
}
```

### Model Management

**GET `/api/models/registry`** - All discovered models
**GET `/api/models/servers`** - Running server status
**POST `/api/models/rescan`** - Re-scan for new models
**PUT `/api/models/{model_id}/enabled`** - Enable/disable (auto-starts/stops server)

Full API docs: http://localhost:8000/docs

## Configuration

### Environment Variables

```bash
# Model Discovery
MODEL_SCAN_PATH=/Users/you/.cache/huggingface/hub/
LLAMA_SERVER_PATH=/usr/local/bin/llama-server
MODEL_PORT_RANGE_START=8080
MODEL_PORT_RANGE_END=8099

# CGRAG
CGRAG_INDEX_PATH=/data/faiss_indexes/
CGRAG_CHUNK_SIZE=512
CGRAG_TOKEN_BUDGET=8000
CGRAG_MIN_RELEVANCE=0.7

# Query Modes
DEFAULT_QUERY_MODE=two-stage
```

### Indexing Documentation for CGRAG

```bash
# One-time setup
docker-compose run --rm backend python -m app.cli.index_docs /path/to/docs

# Indexes are saved to ./backend/data/faiss_indexes/
```

## Development

### Hot Reload Enabled by Default

Code changes are automatically detected:

```bash
# Edit backend Python files ‚Üí Uvicorn auto-reloads
# Edit frontend React/TypeScript ‚Üí Vite HMR updates browser
```

### Rebuilding After Dependency Changes

```bash
# Backend dependencies changed (requirements.txt)
docker-compose build --no-cache backend
docker-compose up -d

# Frontend dependencies changed (package.json)
docker-compose build --no-cache frontend
docker-compose up -d
```

### View Logs

```bash
docker-compose logs -f backend
docker-compose logs -f frontend
```

## Troubleshooting

**Docker starts but no models visible?**
- Check `MODEL_SCAN_PATH` in .env points to HuggingFace cache
- Navigate to Model Management ‚Üí Click "RE-SCAN HUB"

**Models enabled but won't start?**
- Verify llama-server exists: `ls /usr/local/bin/llama-server`
- Check backend logs: `docker-compose logs -f backend`
- Ensure ports 8080-8099 are available

**CGRAG returns no results?**
- Index your documentation first: `docker-compose run --rm backend python -m app.cli.index_docs /docs`
- Verify index exists: `ls ./backend/data/faiss_indexes/`

**Two-stage mode not working?**
- Ensure you have both BALANCED and POWERFUL tier models enabled
- Check backend logs for routing errors

## Future Vision

### Council Mode (Planned)
- Query sent to 3-5 models simultaneously
- Each proposes independent response
- Models "discuss" via sequential refinement
- Final consensus response

### Debate Mode (Planned)
- Two models argue opposing viewpoints
- Alternating responses with reasoning
- User judges or system synthesizes

### Multi-Chat Mode (Planned)
- Models engage in conversation
- Different personas/perspectives
- User moderates discussion

## Performance Targets

- ‚úÖ Docker startup: <5 seconds (no models)
- ‚úÖ Model startup: 40-50 seconds (4 models concurrently)
- ‚úÖ Simple query: <2 seconds (FAST tier)
- ‚úÖ Two-stage query: <15 seconds total
- ‚úÖ CGRAG retrieval: <100ms
- ‚úÖ UI animations: 60fps

## Contributing

1. Follow Docker-only development workflow
2. Test changes in Docker environment
3. Update documentation with changes
4. Test CGRAG retrieval after doc changes
5. Validate two-stage workflow if modifying routing

## License

[Your License]

---

**Last Updated:** November 3, 2025
**Version:** 3.0 (WebUI Experimentation Platform)
**CGRAG Status:** Production-Ready (2,351 lines)
**Two-Stage Status:** Implemented ‚úÖ
**Dynamic Control:** Implemented ‚úÖ
```

---

## Phase 12: Supporting Documentation

### New File: [docs/features/MODES.md](../features/MODES.md)

Create this file to document all query modes:

```markdown
# MAGI Query Modes

## Overview

MAGI supports multiple query processing modes, each optimized for different use cases.

## Available Modes

### ‚úÖ Two-Stage Mode (Implemented)

**Purpose:** Balance speed and quality through sequential refinement

**Workflow:**
1. Fast model (BALANCED tier) with CGRAG context generates initial response
2. Powerful model (POWERFUL tier) refines the initial response

**Best for:**
- Complex queries requiring depth
- Questions benefiting from context retrieval
- Balanced speed/quality trade-off

**Performance:** <15 seconds total

### ‚úÖ Simple Mode (Implemented)

**Purpose:** Single model query for straightforward requests

**Workflow:**
1. Single model processes query directly
2. Optional CGRAG context retrieval

**Best for:**
- Simple queries
- When speed is critical
- Testing individual models

**Performance:** <5 seconds

### üîÑ Council Mode (Coming Soon)

**Purpose:** Multiple models collaborate to reach consensus

**Planned Workflow:**
1. Query sent to 3-5 models simultaneously
2. Each generates independent response
3. Models review each other's responses
4. Iterative refinement toward consensus
5. Final synthesized response

**Best for:**
- Critical decisions requiring multiple perspectives
- Complex analysis
- Reducing model bias

### üîÑ Debate Mode (Coming Soon)

**Purpose:** Two models argue opposing viewpoints

**Planned Workflow:**
1. Query framed as debate topic
2. Model A presents one position
3. Model B presents opposing position
4. Alternating rebuttals
5. User judges or system synthesizes

**Best for:**
- Exploring multiple angles
- Identifying weaknesses in arguments
- Creative brainstorming

### üîÑ Multi-Chat Mode (Coming Soon)

**Purpose:** Models engage in conversation with each other

**Planned Workflow:**
1. User provides conversation topic
2. Models assigned different personas/roles
3. Models exchange messages
4. User moderates/steers discussion
5. Conversation summary generated

**Best for:**
- Brainstorming sessions
- Role-playing scenarios
- Creative writing

## Mode Selection

Use the Mode Selector in the WebUI Home page to choose your query mode.

## Future Enhancements

- Custom mode parameters (number of models, iteration count)
- Hybrid modes (combine multiple approaches)
- User-defined workflows
```

### New File: [docs/features/DYNAMIC_CONTROL.md](../features/DYNAMIC_CONTROL.md)

```markdown
# Dynamic Model Control

## Overview

MAGI allows starting, stopping, and managing LLM servers dynamically from the WebUI without restarting Docker.

## Key Benefits

1. **Fast Startup** - Docker launches in ~5 seconds with no models
2. **Flexible** - Load only the models you need, when you need them
3. **Resource Efficient** - Stop models to free memory
4. **No Restart** - All changes happen live

## How It Works

### Discovery vs. Activation

MAGI separates discovery from activation:

- **Discovery:** Scans HuggingFace cache, finds all GGUF models (automatic, <5s)
- **Activation:** Launches llama-server processes (user-controlled, 10-15s per model)

### User Workflow

1. **Model Management Page** ‚Üí View all discovered models
2. **Check "Enabled"** on desired models
3. **Click "START ALL ENABLED"** ‚Üí Servers launch dynamically
4. **Use models** in query modes
5. **Click "STOP ALL SERVERS"** when done ‚Üí Free memory

### API-Driven Control

Frontend sends REST API calls:
- `POST /api/models/servers/start-all` ‚Üí Start all enabled models
- `POST /api/models/servers/{model_id}/start` ‚Üí Start specific model
- `POST /api/models/servers/{model_id}/stop` ‚Üí Stop specific model
- `POST /api/models/servers/stop-all` ‚Üí Stop all servers

Backend manages llama.cpp processes:
- Concurrent startup (parallel loading)
- Health check validation
- Graceful shutdown (SIGTERM ‚Üí SIGKILL)

## Performance

- **Individual model startup:** 10-15 seconds
- **4 models concurrently:** 40-50 seconds
- **Server stop:** <5 seconds
- **Docker restart:** Not required!

## Use Cases

### Development
Load one fast model for quick testing, stop when done.

### Experimentation
Try different quantizations (Q2, Q3, Q4) without restarting.

### Production
Load powerful models only when needed, save resources during idle time.

### Benchmarking
Start/stop models to compare performance in isolation.
```

---

## Testing Checklist

After implementation, verify:

### ‚úÖ Phase 1 - Startup
- [ ] Docker starts in <10 seconds
- [ ] No llama-server processes running after startup
- [ ] `docker ps` shows backend/frontend/redis only
- [ ] Backend logs show "NO models loaded (awaiting WebUI commands)"

### ‚úÖ Phase 2 - Dynamic Control API
- [ ] `POST /api/models/servers/{model_id}/start` successfully starts server
- [ ] `GET /api/models/servers` shows server in running status
- [ ] `POST /api/models/servers/{model_id}/stop` successfully stops server
- [ ] `POST /api/models/servers/start-all` starts all enabled models
- [ ] `POST /api/models/servers/stop-all` stops all running servers

### ‚úÖ Phase 3 - Frontend Controls
- [ ] Model Management page shows START ALL / STOP ALL buttons
- [ ] Clicking START ALL launches servers (see progress in logs)
- [ ] Model checkboxes show running/stopped status
- [ ] Clicking STOP ALL terminates servers
- [ ] Enabling model checkbox auto-starts server

### ‚úÖ Phase 4 - Mode Selector
- [ ] Home page shows Mode Selector component
- [ ] Two-Stage mode is selectable and marked ACTIVE
- [ ] Simple mode is selectable
- [ ] Council, Debate, Chat modes show "COMING SOON"
- [ ] Clicking disabled modes does nothing

### ‚úÖ Phase 5 - Two-Stage Workflow
- [ ] Selecting Two-Stage mode and submitting query triggers two-stage processing
- [ ] Backend logs show Stage 1 (balanced tier) execution
- [ ] Backend logs show Stage 2 (powerful tier) execution
- [ ] Response includes stage1Response in metadata
- [ ] Total processing time = stage1 + stage2

### ‚úÖ Phase 6 - Two-Stage Display
- [ ] Response shows "TWO-STAGE PROCESSING" panel
- [ ] Panel shows Stage 1 details (tier, model, time)
- [ ] Panel shows Stage 2 details (tier, model, time)
- [ ] Clicking "View Stage 1 Response" expands full text
- [ ] Total timing displayed at bottom

### ‚úÖ Phase 7 - Deprecated Code Removal
- [ ] No Q2/Q3/Q4 references in backend logs
- [ ] grep -r "Q2_FAST" backend/ returns nothing
- [ ] grep -r "model_q2_" backend/app/core/config.py returns nothing
- [ ] All tier references use "fast", "balanced", "powerful"

### ‚úÖ Phase 8 - Docker Optimization
- [ ] docker-compose.dev.yml deleted
- [ ] docker-compose.yml includes hot reload by default
- [ ] Backend image size <700MB (check with `docker images`)
- [ ] Hot reload works (edit Python file, see auto-reload)
- [ ] Frontend HMR works (edit React component, see instant update)

### ‚úÖ End-to-End
- [ ] Complete flow: Start Docker ‚Üí Enable models ‚Üí Start servers ‚Üí Two-stage query ‚Üí Response
- [ ] Complete flow: Stop servers ‚Üí Re-enable different models ‚Üí Restart ‚Üí Simple query
- [ ] CGRAG still works (<100ms retrieval)
- [ ] Model discovery still works (RE-SCAN HUB)
- [ ] Admin panel endpoints still work

---

## Implementation Order

**Recommended sequence:**

1. **Phase 1** (Remove auto-start) - Foundational change
2. **Phase 2** (Dynamic API) - Backend infrastructure
3. **Phase 3** (Frontend buttons) - User control
4. **Test:** Verify dynamic start/stop works end-to-end
5. **Phase 4** (Mode selector) - UI framework
6. **Phase 5** (Two-stage backend) - Core logic
7. **Phase 6** (Two-stage frontend) - Display
8. **Test:** Verify two-stage workflow end-to-end
9. **Phase 7** (Clean deprecated) - Code cleanup
10. **Phase 8** (Docker) - Infrastructure optimization
11. **Phase 11** (README) - Documentation
12. **Phase 12** (Supporting docs) - Additional docs

---

## Expected Results

### User Experience

**Before:**
```
docker-compose up -d
  ‚Üì
Wait 40-50 seconds (all models launch)
  ‚Üì
Visit WebUI
  ‚Üì
Can only use enabled models from YAML
  ‚Üì
To change models: Edit YAML, restart Docker, wait 40-50s
```

**After:**
```
docker-compose up -d
  ‚Üì
Wait 5 seconds (discovery only)
  ‚Üì
Visit WebUI ‚Üí Model Management
  ‚Üì
Enable desired models ‚Üí Click START ALL
  ‚Üì
Wait 40-50 seconds (dynamic launch)
  ‚Üì
Select mode (Two-Stage, Simple, etc.) ‚Üí Query
  ‚Üì
To change models: Click STOP ALL, enable different models, START ALL
(No Docker restart!)
```

### System Metrics

| Metric | Before | After |
|--------|--------|-------|
| Docker startup | 40-50s | 5s |
| Change models | Restart Docker (40-50s) | Click button (<5s stop + 40-50s start) |
| Memory usage (idle) | High (all models loaded) | Low (no models) |
| Flexibility | Profile-locked | WebUI-driven |
| Modes | Single/hardcoded two-stage | 5 modes (2 implemented) |

---

## Notes for Implementation

### Critical Sections

1. **main.py lifespan:** Be careful not to break CGRAG preloading
2. **query.py routing:** Preserve existing CGRAG integration
3. **Frontend types:** Ensure camelCase/snake_case conversion correct

### Backward Compatibility

The following should still work:
- Existing `/api/query` endpoint with `mode: "simple"`
- CGRAG retrieval (<100ms)
- Model discovery/re-scan
- Admin panel endpoints
- Health checks

### Testing Environment

Recommend testing with:
- 2-3 GGUF models of different sizes
- Documentation directory for CGRAG indexing
- Docker Desktop with 8GB RAM allocation

---

## Troubleshooting Guide

### Issue: Servers don't start

**Check:**
1. llama-server binary exists: `ls /usr/local/bin/llama-server`
2. Ports available: `lsof -i :8080-8099`
3. Backend logs: `docker-compose logs -f backend`
4. Model file exists and readable

### Issue: Hot reload not working

**Check:**
1. Volume mounts in docker-compose.yml
2. `--reload` flag in uvicorn command
3. File permissions (ownership issues)

### Issue: Two-stage fails

**Check:**
1. Both BALANCED and POWERFUL tier models enabled and running
2. Backend logs for specific error
3. Model selection logic (tier availability)

### Issue: Frontend doesn't update

**Check:**
1. Vite dev server running: `docker-compose logs -f frontend`
2. HMR WebSocket connection (browser console)
3. Build args passed correctly

---

## Success Criteria

Implementation is complete when:

1. ‚úÖ Docker starts in <10 seconds with no models
2. ‚úÖ User can start/stop models from WebUI without restart
3. ‚úÖ Two-stage workflow processes queries correctly
4. ‚úÖ Mode selector displays all modes (2 available, 3 coming soon)
5. ‚úÖ CGRAG still retrieves context <100ms
6. ‚úÖ No Q2/Q3/Q4 references in codebase
7. ‚úÖ Backend Docker image <700MB
8. ‚úÖ Hot reload works for both backend and frontend
9. ‚úÖ README accurately describes new behavior
10. ‚úÖ All existing features still work

---

**Document Version:** 1.0
**Created:** November 3, 2025
**Implementation Time:** 10-12 hours estimated
**Status:** Ready for implementation

---

**Good luck with the implementation! üöÄ**
