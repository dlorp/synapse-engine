# =============================================================================
# S.Y.N.A.P.S.E. ENGINE - Environment Configuration Template
# =============================================================================
# Copy this file to .env and fill in your values.
# NEVER commit .env to version control - it contains secrets.

# -----------------------------------------------------------------------------
# CORE:PRAXIS Profile Configuration
# -----------------------------------------------------------------------------
# Active profile: development, production, fast-only, or custom profile name
PRAXIS_PROFILE=development

# -----------------------------------------------------------------------------
# Environment
# -----------------------------------------------------------------------------
ENVIRONMENT=development  # development | staging | production
LOG_LEVEL=DEBUG          # DEBUG | INFO | WARNING | ERROR

# -----------------------------------------------------------------------------
# CORE:PRAXIS Configuration (FastAPI Orchestrator)
# -----------------------------------------------------------------------------
PRAXIS_HOST=0.0.0.0
PRAXIS_PORT=8000

# -----------------------------------------------------------------------------
# CORE:MEMEX Configuration (Redis Cache & Session Store)
# -----------------------------------------------------------------------------
MEMEX_HOST=synapse_redis
MEMEX_PORT=6379
MEMEX_PASSWORD=change_this_secure_redis_password  # CHANGE THIS IN PRODUCTION
MEMEX_DB=0
MEMEX_TTL=3600
MEMEX_URL=redis://:${MEMEX_PASSWORD}@${MEMEX_HOST}:${MEMEX_PORT}/${MEMEX_DB}

# -----------------------------------------------------------------------------
# CORE:PRAXIS Model Management
# -----------------------------------------------------------------------------
# Directory containing GGUF model files
PRAXIS_MODEL_PATH=${PRAXIS_MODEL_PATH}/

# Path to llama-server binary
PRAXIS_LLAMA_SERVER_PATH=/usr/local/bin/llama-server

# Model registry cache file
PRAXIS_REGISTRY_PATH=data/model_registry.json

# Port range for model servers
PRAXIS_PORT_RANGE_START=8080
PRAXIS_PORT_RANGE_END=8099

# Server startup settings
PRAXIS_MAX_STARTUP_TIME=120
PRAXIS_CONCURRENT_STARTS=true

# =============================================================================
# NODE:NEURAL Configuration (Host API - Metal Server Management) v4.0
# =============================================================================
# Automatic Metal-accelerated llama-server management via Host API service
#
# WHAT IT DOES:
# - Launches native Metal-accelerated llama-servers on macOS host (2-3x faster)
# - One-click startup from WebUI (no manual terminal commands)
# - Automatic shutdown when Docker stops (no orphaned processes)
# - Real-time logs in System Logs panel
#
# REQUIREMENTS:
# 1. macOS with Apple Silicon (M1/M2/M3/M4)
# 2. llama.cpp installed: brew install llama.cpp
# 3. SSH configured (see README.md Metal Acceleration section)
#
# SETUP (One-Time):
# 1. Generate SSH key:
#    ssh-keygen -t ed25519 -f ~/.ssh/synapse_host_api -N ""
#
# 2. Add to authorized_keys with command restriction:
#    echo "command=\"/opt/homebrew/bin/bash $(pwd)/scripts/ssh-wrapper.sh\" $(cat ~/.ssh/synapse_host_api.pub)" >> ~/.ssh/authorized_keys
#
# 3. Copy keys to host-api:
#    mkdir -p host-api/.ssh
#    cp ~/.ssh/synapse_host_api host-api/.ssh/id_ed25519
#    cp ~/.ssh/synapse_host_api.pub host-api/.ssh/id_ed25519.pub
#
# 4. Create SSH config:
#    cat > host-api/.ssh/config <<EOF
#    Host mac-host
#        HostName host.docker.internal
#        User $USER
#        IdentityFile ~/.ssh/id_ed25519
#        StrictHostKeyChecking no
#        UserKnownHostsFile /dev/null
#    EOF
#
# 5. Start S.Y.N.A.P.S.E. ENGINE: docker-compose up -d
#
# USAGE:
# - Go to Model Management in WebUI
# - Enable models you want to use
# - Click "START ALL ENABLED"
# - Watch System Logs for real-time progress
# - Models launch with Metal GPU acceleration automatically!
#
# PERFORMANCE:
# - Startup: 3-5 seconds per model (vs 15-30s CPU-only)
# - Inference: 2-3x faster than CPU
# - VRAM: Uses unified memory efficiently
#
# SECURITY:
# - SSH key-only authentication (no passwords)
# - Command restriction via authorized_keys
# - Only start-metal-servers and stop-metal-servers allowed
# - No arbitrary command execution possible
#
# TROUBLESHOOTING:
# - "SSH connection failed" → Check SSH keys and authorized_keys
# - "Unauthorized command" → Verify ssh-wrapper.sh is in authorized_keys
# - "Can't connect to servers" → Check host.docker.internal is reachable
# - See README.md Metal Acceleration section for detailed troubleshooting
#
# TO DISABLE (use CPU-only mode instead):
# Set NEURAL_USE_EXTERNAL=false and restart synapse_core
# =============================================================================

# Enable/disable Metal acceleration via Host API
# true = Metal GPU (fast, macOS only, requires SSH setup)
# false = CPU-only inside Docker (slower, portable to Linux)
NEURAL_USE_EXTERNAL=true

# Host API service URL (for automatic Metal server management)
# DO NOT CHANGE unless you modify docker-compose.yml service name
NEURAL_ORCH_URL=http://synapse_host_api:9090

# -----------------------------------------------------------------------------
# Legacy Model Server URLs (for backward compatibility)
# -----------------------------------------------------------------------------
# These are used by the legacy ModelManager for the /status endpoint
# The new model management system uses discovery + profiles instead
# macOS/Windows: Use host.docker.internal
# Linux: Use 172.17.0.1 or add extra_hosts in docker-compose.yml
MODEL_Q2_FAST_1_URL=http://host.docker.internal:8080
MODEL_Q2_FAST_2_URL=http://host.docker.internal:8081
MODEL_Q3_SYNTH_URL=http://host.docker.internal:8082
MODEL_Q4_DEEP_URL=http://host.docker.internal:8083

# -----------------------------------------------------------------------------
# NODE:RECALL Configuration (CGRAG - Contextual Retrieval)
# -----------------------------------------------------------------------------
# FAISS index storage path (absolute path within container)
RECALL_FAISS_DEFAULT=/app/data/faiss_indexes/default

# Base path for all RECALL indexes (directory)
RECALL_INDEX_PATH=/app/data/faiss_indexes

# Sentence transformer model for embeddings
RECALL_EMBEDDING_MODEL=all-MiniLM-L6-v2

# Enable caching for embeddings to improve performance
RECALL_EMBEDDING_CACHE=true

# Chunking configuration for document processing
RECALL_CHUNK_SIZE=512
RECALL_CHUNK_OVERLAP=50

# Retrieval parameters
RECALL_TOKEN_BUDGET=8000
RECALL_MIN_RELEVANCE=0.7
RECALL_MAX_SHARDS=10

# -----------------------------------------------------------------------------
# Query Modes Configuration
# -----------------------------------------------------------------------------
# Default query mode for new queries
# Options: simple, two-stage, council, debate, chat
DEFAULT_QUERY_MODE=two-stage

# Enable experimental/advanced query modes
# When false, only simple and two-stage modes are available
ENABLE_EXPERIMENTAL_MODES=false

# Mode-specific settings
# Council mode: Number of models to consult in parallel
COUNCIL_MODE_MODEL_COUNT=3

# Debate mode: Number of debate rounds
DEBATE_MODE_ROUNDS=2

# Chat mode: Maximum conversation history length (in messages)
CHAT_MODE_MAX_HISTORY=20

# -----------------------------------------------------------------------------
# CORE:INTERFACE Configuration (React Frontend)
# -----------------------------------------------------------------------------
# IMPORTANT: These must be available at BUILD TIME for Vite to embed them
# into the JavaScript bundle. For development with docker-compose.yml,
# these are passed as runtime environment variables to Vite dev server.
# For production builds, pass them as build args in docker-compose.yml.
IFACE_API_BASE_URL=/api
IFACE_WS_URL=/ws
IFACE_PORT=5173

# -----------------------------------------------------------------------------
# Security
# -----------------------------------------------------------------------------
# CORS allowed origins (comma-separated)
CORS_ORIGINS=http://localhost:5173,http://localhost:3000
