# =============================================================================
# S.Y.N.A.P.S.E. ENGINE - Enhanced CGRAG Configuration
# =============================================================================
# Environment variables for the enhanced CGRAG system with:
# - Hybrid Search (BM25 + Vector)
# - Qdrant/FAISS vector database
# - Cross-encoder reranking
# - Knowledge graph enhancement
# - Multi-context sources (docs, code, chat history)
#
# Usage:
#   1. Copy this file to .env.cgrag
#   2. Customize values for your environment
#   3. Load in docker-compose.yml or source before running
#   4. Restart services: docker-compose restart synapse_core
# =============================================================================

# =============================================================================
# Vector Database Configuration
# =============================================================================

# Vector database backend selection
# Options: auto | qdrant | faiss
# - auto: Use Qdrant on Metal-enabled systems, FAISS otherwise
# - qdrant: Always use Qdrant (requires qdrant service running)
# - faiss: Always use FAISS (CPU-based, no external dependencies)
RECALL_VECTOR_DB=auto

# Qdrant connection settings (if RECALL_VECTOR_DB=qdrant or auto)
RECALL_QDRANT_URL=http://synapse_qdrant:6333
RECALL_QDRANT_GRPC_URL=http://synapse_qdrant:6334
RECALL_QDRANT_COLLECTION=synapse_cgrag
RECALL_QDRANT_DISTANCE_METRIC=cosine  # cosine | euclid | dot

# Qdrant performance tuning
RECALL_QDRANT_SEARCH_LIMIT=100  # Max candidates to retrieve
RECALL_QDRANT_EF_SEARCH=128     # HNSW search parameter (higher = more accurate, slower)
RECALL_QDRANT_M=16              # HNSW graph connectivity (higher = more accurate, more memory)

# FAISS configuration (if RECALL_VECTOR_DB=faiss or fallback)
RECALL_FAISS_INDEX_TYPE=IVF     # Flat | IVF | HNSW
RECALL_FAISS_NPROBE=10          # Number of clusters to search (IVF only)
RECALL_FAISS_NLIST=100          # Number of clusters (IVF only)

# =============================================================================
# Hybrid Search Configuration
# =============================================================================

# Enable hybrid search (BM25 + Vector)
RECALL_HYBRID_SEARCH=true

# Hybrid search weight (0.0 = BM25 only, 1.0 = Vector only, 0.5 = balanced)
# Recommended: 0.7 (favor vector search, use BM25 for keyword matches)
RECALL_HYBRID_ALPHA=0.7

# Reciprocal Rank Fusion (RRF) parameter
# Lower k = less influence from lower-ranked results
# Typical range: 20-100
RECALL_HYBRID_RRF_K=60

# Number of candidates to retrieve from each method before fusion
RECALL_HYBRID_VECTOR_TOP_K=50
RECALL_HYBRID_BM25_TOP_K=50

# BM25 parameters (Okapi BM25)
RECALL_BM25_K1=1.5              # Term frequency saturation (typical: 1.2-2.0)
RECALL_BM25_B=0.75              # Length normalization (typical: 0.5-1.0)

# =============================================================================
# Reranker Configuration
# =============================================================================

# Enable cross-encoder reranking (2-stage retrieval)
RECALL_RERANKER_ENABLED=true

# Reranker model (HuggingFace cross-encoder)
# Options:
# - cross-encoder/ms-marco-MiniLM-L-6-v2 (fast, 80MB)
# - cross-encoder/ms-marco-MiniLM-L-12-v2 (balanced, 120MB)
# - cross-encoder/ms-marco-electra-base (accurate, 400MB)
RECALL_RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# Number of candidates to rerank
# Retrieve more candidates, rerank top N for accuracy
# Typical: 50-200
RECALL_RERANKER_TOP_K=100

# Final number of results to return after reranking
# Typical: 5-20
RECALL_RERANKER_FINAL_TOP_K=10

# Reranker batch size (for GPU acceleration)
# Increase if using GPU, decrease if CPU only
RECALL_RERANKER_BATCH_SIZE=32

# Reranker device
# Options: auto | cpu | cuda | mps
RECALL_RERANKER_DEVICE=auto

# =============================================================================
# Knowledge Graph Configuration
# =============================================================================

# Enable knowledge graph enhancement
RECALL_KG_ENABLED=true

# Entity extraction method
# Options:
# - spacy (fast, rule-based, good for code)
# - transformers (accurate, NER model, slower)
RECALL_KG_ENTITY_EXTRACTION=spacy

# SpaCy model (if RECALL_KG_ENTITY_EXTRACTION=spacy)
# Options: en_core_web_sm | en_core_web_md | en_core_web_lg
RECALL_KG_SPACY_MODEL=en_core_web_sm

# Transformer model (if RECALL_KG_ENTITY_EXTRACTION=transformers)
RECALL_KG_NER_MODEL=dslim/bert-base-NER

# Graph database backend
# Options: networkx | neo4j
# - networkx: In-memory, fast, no external dependencies
# - neo4j: Persistent, scalable, requires neo4j service
RECALL_KG_BACKEND=networkx

# Neo4j connection (if RECALL_KG_BACKEND=neo4j)
RECALL_KG_NEO4J_URL=bolt://localhost:7687
RECALL_KG_NEO4J_USER=neo4j
RECALL_KG_NEO4J_PASSWORD=change_this_password

# Graph traversal depth
# How many hops to follow when expanding context
# Typical: 1-3
RECALL_KG_MAX_HOPS=2

# Minimum edge weight (relevance threshold)
# Edges below this weight are ignored during traversal
RECALL_KG_MIN_EDGE_WEIGHT=0.5

# =============================================================================
# Embedding Configuration
# =============================================================================

# Embedding model (sentence-transformers)
# Options:
# - all-MiniLM-L6-v2 (fast, 384d, 80MB)
# - all-mpnet-base-v2 (balanced, 768d, 420MB)
# - e5-large-v2 (accurate, 1024d, 1.3GB)
RECALL_EMBEDDING_MODEL=all-MiniLM-L6-v2

# Embedding batch size
# Larger = faster indexing, more memory
RECALL_EMBEDDING_BATCH_SIZE=64

# Embedding device
# Options: auto | cpu | cuda | mps
RECALL_EMBEDDING_DEVICE=auto

# Enable embedding cache (Redis)
RECALL_EMBEDDING_CACHE=true

# Embedding cache TTL (seconds)
# -1 = no expiration
RECALL_EMBEDDING_CACHE_TTL=86400  # 24 hours

# Prefetch embeddings for hot data
# Precompute embeddings for frequently accessed documents
RECALL_PREFETCH_EMBEDDINGS=true

# =============================================================================
# Context Sources Configuration
# =============================================================================

# Enable multi-context source registry
RECALL_MULTI_CONTEXT=true

# Context sources to enable (comma-separated)
# Options: docs, codebase, chat_history, web
# Note: web search disabled per privacy requirements
RECALL_CONTEXT_SOURCES=docs,codebase,chat_history

# Documentation context
RECALL_CONTEXT_DOCS_ENABLED=true
RECALL_CONTEXT_DOCS_PATH=./docs
RECALL_CONTEXT_DOCS_EXTENSIONS=.md,.txt,.rst

# Codebase context
RECALL_CONTEXT_CODE_ENABLED=true
RECALL_CONTEXT_CODE_PATH=./backend,./frontend/src
RECALL_CONTEXT_CODE_EXTENSIONS=.py,.ts,.tsx,.js,.jsx

# Chat history context
RECALL_CONTEXT_CHAT_ENABLED=true
RECALL_CONTEXT_CHAT_MAX_MESSAGES=100
RECALL_CONTEXT_CHAT_SESSION_TIMEOUT=3600  # 1 hour

# =============================================================================
# Chunking Configuration
# =============================================================================

# Text chunking strategy
# Options:
# - fixed: Fixed-size chunks with overlap
# - semantic: Sentence-aware chunking
# - ast: AST-based code chunking
# - markdown: Structure-aware Markdown chunking
RECALL_CHUNKING_STRATEGY=semantic

# Chunk size (characters)
# Larger = more context per chunk, fewer chunks
# Smaller = more precise retrieval, more chunks
RECALL_CHUNK_SIZE=512

# Chunk overlap (characters)
# Prevents context loss at chunk boundaries
# Typical: 10-20% of chunk_size
RECALL_CHUNK_OVERLAP=50

# AST-based code chunking (if RECALL_CHUNKING_STRATEGY=ast)
RECALL_AST_ENABLED=true
RECALL_AST_MIN_SIZE=100          # Minimum chunk size (chars)
RECALL_AST_MAX_SIZE=1000         # Maximum chunk size (chars)
RECALL_AST_PRESERVE_STRUCTURE=true  # Keep function/class intact

# =============================================================================
# Token Budget Management
# =============================================================================

# Token budget for context retrieval
# Must fit within model context window
# Typical: 4000-8000 for 8k context window
RECALL_TOKEN_BUDGET=8000

# Minimum relevance threshold
# Chunks below this score are excluded
# Range: 0.0-1.0, typical: 0.6-0.8
RECALL_MIN_RELEVANCE=0.7

# Reserve tokens for query/response
# Budget allocated before context insertion
RECALL_RESERVED_TOKENS=2000

# =============================================================================
# Performance Tuning
# =============================================================================

# Async indexing (background processing)
# Index new documents without blocking queries
RECALL_ASYNC_INDEXING=true

# Indexing queue size
RECALL_INDEXING_QUEUE_SIZE=1000

# Indexing workers
# Number of parallel workers for indexing
RECALL_INDEXING_WORKERS=4

# Query cache TTL (seconds)
# Cache retrieval results for identical queries
RECALL_QUERY_CACHE_TTL=300  # 5 minutes

# Query cache max size (entries)
RECALL_QUERY_CACHE_MAX_SIZE=1000

# =============================================================================
# Monitoring & Metrics
# =============================================================================

# Enable Prometheus metrics
PROMETHEUS_ENABLED=true

# Metrics port (exposed on /metrics endpoint)
PROMETHEUS_PORT=8000

# Metrics prefix
PROMETHEUS_PREFIX=cgrag_

# Enable detailed metrics (includes histograms, summaries)
# Warning: Increases memory usage
PROMETHEUS_DETAILED_METRICS=true

# Histogram buckets for latency metrics (seconds)
PROMETHEUS_LATENCY_BUCKETS=0.005,0.01,0.025,0.05,0.1,0.25,0.5,1.0,2.5,5.0

# =============================================================================
# Logging Configuration
# =============================================================================

# CGRAG log level
# Options: DEBUG | INFO | WARNING | ERROR | CRITICAL
RECALL_LOG_LEVEL=INFO

# Log format
# Options: json | text
RECALL_LOG_FORMAT=json

# Log file path (set to empty to disable file logging)
RECALL_LOG_FILE=/app/logs/cgrag.log

# Log rotation
RECALL_LOG_MAX_SIZE=100MB
RECALL_LOG_MAX_FILES=10

# =============================================================================
# Advanced Settings
# =============================================================================

# Index sharding
# Shard large indexes for parallel processing
RECALL_MAX_SHARDS=10
RECALL_SHARD_SIZE=100000  # docs per shard

# Index optimization
# Rebuild index periodically for performance
RECALL_AUTO_OPTIMIZE=true
RECALL_OPTIMIZE_INTERVAL=86400  # 24 hours

# Memory limits
# Maximum memory for FAISS index cache (MB)
RECALL_FAISS_CACHE_SIZE=2048  # 2GB

# Qdrant memory limit (MB)
RECALL_QDRANT_CACHE_SIZE=3072  # 3GB

# =============================================================================
# Development & Debugging
# =============================================================================

# Enable debug mode
# WARNING: Verbose logging, performance impact
RECALL_DEBUG=false

# Enable query profiling
# Track query execution time breakdown
RECALL_PROFILE_QUERIES=false

# Enable index validation
# Verify index integrity on startup
RECALL_VALIDATE_INDEX_ON_STARTUP=true

# Enable test endpoints
# Expose /api/cgrag/test-* endpoints for debugging
RECALL_ENABLE_TEST_ENDPOINTS=true

# =============================================================================
# Notes:
# =============================================================================
# 1. Hybrid Search Trade-offs:
#    - Higher alpha (>0.7): Better semantic matching, may miss exact keywords
#    - Lower alpha (<0.5): Better keyword matching, may miss semantic similarity
#    - Balanced (0.5): Equal weight, good starting point
#
# 2. Reranker Performance:
#    - Smaller models (<100MB): <50ms latency, good for real-time
#    - Larger models (>400MB): >100ms latency, best accuracy
#    - GPU acceleration: 3-5x faster than CPU
#
# 3. Knowledge Graph Benefits:
#    - Improved multi-hop reasoning
#    - Better entity-centric queries
#    - Contextual disambiguation
#    - Cost: +20-50ms latency, +100-500MB memory
#
# 4. Token Budget Guidelines:
#    - 4k context: budget 2000-3000 tokens for context
#    - 8k context: budget 4000-6000 tokens for context
#    - 16k context: budget 8000-12000 tokens for context
#    - Reserve 25-50% for query + response
#
# 5. Performance Optimization:
#    - Enable async indexing for large document sets
#    - Use query cache for repeated queries
#    - Enable embedding cache to reduce computation
#    - Monitor with Prometheus metrics
#
# 6. Production Recommendations:
#    - Use Qdrant for scalability (>100k documents)
#    - Enable reranker for accuracy (+15-25% improvement)
#    - Enable knowledge graph for complex reasoning
#    - Set up monitoring and alerting (Grafana dashboards)
#    - Schedule index optimization (daily/weekly)
#    - Implement backup automation (./scripts/backup-cgrag-indexes.sh)
# =============================================================================
